{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import util\n",
    "import gan_util\n",
    "import gan\n",
    "import dataset_util\n",
    "import mnist_dataset\n",
    "\n",
    "reload(util);\n",
    "reload(gan_util);\n",
    "reload(gan);\n",
    "reload(dataset_util);\n",
    "reload(mnist_dataset);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_digits(digits):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(\n",
    "        np.concatenate([x for x in np.squeeze(digits)], axis=1), \n",
    "        cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def make_generator(noise, label, is_training):\n",
    "    with slim.arg_scope(\n",
    "        [layers.conv2d_transpose],\n",
    "        activation_fn=tf.nn.relu,\n",
    "        normalizer_fn=layers.batch_norm,\n",
    "        normalizer_params={'is_training': is_training,\n",
    "                           'updates_collections': None}):\n",
    "        top = tf.concat(\n",
    "            noise.embed(cont_dims=512, cat_dims=8, name='noise') +\n",
    "            label.embed(cont_dims=512, cat_dims=8, name='label'), axis=1)\n",
    "        top = tf.reshape(top, [-1, 1, 1, util.get_flatten_dim(top)])\n",
    "        top = layers.conv2d_transpose(top, 128, [3, 3], stride=1, padding='VALID')\n",
    "        top = layers.conv2d_transpose(top, 64, [5, 5], stride=1, padding='VALID')\n",
    "        top = layers.conv2d_transpose(top, 32, [5, 5], stride=2, padding='SAME')\n",
    "        top = layers.conv2d_transpose(top, 1, [5, 5], stride=2, padding='SAME',\n",
    "                                      activation_fn=None, normalizer_fn=None)\n",
    "        top = tf.sigmoid(top)\n",
    "        \n",
    "        return top\n",
    "\n",
    "\n",
    "def make_discriminator(data, noise, label, is_training):\n",
    "    with slim.arg_scope(\n",
    "        [layers.conv2d],\n",
    "        activation_fn=tf.nn.relu,\n",
    "        normalizer_fn=layers.batch_norm,\n",
    "        normalizer_params={'is_training': is_training,\n",
    "                           'updates_collections': None}):\n",
    "    \n",
    "        top = data\n",
    "        top = layers.conv2d(top, 32, [5, 5], stride=2, padding='SAME')\n",
    "        top = layers.conv2d(top, 64, [5, 5], stride=2, padding='SAME')\n",
    "        top = layers.conv2d(top, 128, [5, 5], stride=1, padding='VALID')\n",
    "        top = layers.flatten(top)\n",
    "        top = layers.fully_connected(top, 1024)\n",
    "        \n",
    "        data_logit = util.linear(top, 1)\n",
    "        \n",
    "        return data_logit, label.linear_output([top], [top])\n",
    "\n",
    "\n",
    "train_dir = 'mnist_logs'\n",
    "batch_size = 32\n",
    "noise_dim = 100\n",
    "with tf.Graph().as_default():\n",
    "    dataset = mnist_dataset.Dataset('../mnist')\n",
    "    ds = dataset_util.repeat_shuffle_batch(\n",
    "        dataset.raw, batch_size=batch_size)\n",
    "    iterator = ds.make_initializable_iterator()\n",
    "    next_elem = iterator.get_next()\n",
    "    image, label = (next_elem[mnist_dataset.IMAGE_KEY],\n",
    "                    next_elem[mnist_dataset.LABEL_KEY])\n",
    "    \n",
    "    real_data = util.set_first_dim(image, batch_size)\n",
    "    real_label = gan_util.Signal(\n",
    "        [],\n",
    "        [\n",
    "            gan_util.CatVector(\n",
    "                util.set_first_dim(label, batch_size), 10)\n",
    "        ])\n",
    "    fake_noise = gan_util.Signal(\n",
    "        [\n",
    "            tf.random_normal(shape=(batch_size, noise_dim))\n",
    "        ], [])\n",
    "    fake_label = real_label\n",
    "    model = gan.make_gan_model(\n",
    "        make_generator,\n",
    "        make_discriminator,\n",
    "        fake_noise,\n",
    "        fake_label,\n",
    "        real_data,\n",
    "        real_label)\n",
    "    \n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    g_train_op, d_train_op = model.train_ops(\n",
    "        tf.train.AdamOptimizer(1.0e-3, beta1=0.5),\n",
    "        tf.train.AdamOptimizer(1.0e-3, beta1=0.5),\n",
    "        global_step)\n",
    "    \n",
    "    tf.summary.scalar('generator_loss', model.generator_loss)\n",
    "    tf.summary.scalar('discriminator_loss', model.discriminator_loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer(), feed_dict=dataset.feed_dict)\n",
    "        sess.run(iterator.initializer, feed_dict=dataset.feed_dict)\n",
    "        \n",
    "        summary_writer = tf.summary.FileWriter(\n",
    "            logdir=train_dir, graph=sess.graph)\n",
    "        \n",
    "        with util.TensorflowQueues(sess):\n",
    "            step = 0\n",
    "            while step < 10000:\n",
    "                _, d_loss, d_losses = sess.run([\n",
    "                    d_train_op,\n",
    "                    model.discriminator_loss,\n",
    "                    model.discriminator_losses])\n",
    "        \n",
    "                _, g_loss, g_losses = sess.run([\n",
    "                    g_train_op,\n",
    "                    model.generator_loss,\n",
    "                    model.generator_losses])\n",
    "            \n",
    "                step = sess.run(global_step)\n",
    "                if step % 100 == 0:\n",
    "                    print 'Global step {}: g_loss = {}, d_loss = {}'.format(\n",
    "                        step, g_loss, d_loss)\n",
    "\n",
    "                    print 'Writing summaries'\n",
    "                    summary_proto = sess.run(summary_op)\n",
    "                    summary_writer.add_summary(summary_proto, global_step=step)\n",
    "                        \n",
    "                    print 'Writing checkpoint'\n",
    "                    saver.save(sess, os.path.join(train_dir, 'model'), global_step=step)\n",
    "                    \n",
    "                    feed_dict = model.test_noise.feed_dict([\n",
    "                        np.random.normal(size=(10, noise_dim))\n",
    "                    ], [])\n",
    "                    feed_dict.update(model.test_label.feed_dict([], [\n",
    "                        np.arange(10)\n",
    "                    ]))\n",
    "                    test_data = sess.run(model.test_data, feed_dict=feed_dict)\n",
    "                    plot_digits(test_data)\n",
    "                    \n",
    "                    print '='*50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "ckpt = tf.train.latest_checkpoint('mnist_logs')\n",
    "saver = tf.train.import_meta_graph(ckpt + '.meta')\n",
    "saver.restore(sess, ckpt)\n",
    "    \n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "print graph.get_collection(gan.TEST_NOISE_KEY)\n",
    "print graph.get_collection(gan.TEST_LABEL_KEY)\n",
    "print graph.get_collection(gan.TEST_DATA_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noise_dim = graph.get_collection(\n",
    "    gan.TEST_NOISE_KEY)[0].get_shape().as_list()[1]\n",
    "\n",
    "feed_dict = {\n",
    "    graph.get_collection(gan.TEST_NOISE_KEY)[0]:\n",
    "        np.random.normal(size=(10, noise_dim)),\n",
    "    graph.get_collection(gan.TEST_LABEL_KEY)[0]:\n",
    "        np.arange(10)[::-1]\n",
    "}\n",
    "\n",
    "test_data = sess.run(graph.get_collection(gan.TEST_DATA_KEY),\n",
    "                     feed_dict=feed_dict)\n",
    "        \n",
    "plot_digits(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
