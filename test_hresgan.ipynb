{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "import util\n",
    "import dataset_util\n",
    "import celeba_dataset\n",
    "\n",
    "reload(util);\n",
    "reload(dataset_util);\n",
    "reload(celeba_dataset);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 256 x 256\n",
    "\n",
    "def make_generator(noise, is_training):\n",
    "    with slim.arg_scope(\n",
    "        [layers.conv2d_transpose],\n",
    "        padding='SAME',\n",
    "        activation_fn=tf.nn.relu,\n",
    "        normalizer_fn=layers.batch_norm,\n",
    "        normalizer_params={'is_training': is_training,\n",
    "                           'updates_collections': None}):\n",
    "        top = layers.flatten(noise)\n",
    "        top = layers.fully_connected(top, 8 * 8 * 512)\n",
    "        top = tf.reshape(top, [-1, 8, 8, 512])\n",
    "        top = layers.conv2d_transpose(top, 256, [5, 5], stride=2)\n",
    "        top = layers.conv2d_transpose(top, 128, [5, 5], stride=2)\n",
    "        top = layers.conv2d_transpose(top, 64, [5, 5], stride=2)\n",
    "        top = layers.conv2d_transpose(top, 32, [5, 5], stride=2)\n",
    "        top = layers.conv2d_transpose(top, 3, [5, 5], stride=2,\n",
    "                                      activation_fn=tf.sigmoid, normalizer_fn=None)\n",
    "        \n",
    "        return top\n",
    "\n",
    "\n",
    "def make_discriminator(data, is_training):\n",
    "    with slim.arg_scope(\n",
    "        [layers.conv2d],\n",
    "        padding='SAME',\n",
    "        activation_fn=util.make_leaky_relu(0.2),\n",
    "        normalizer_fn=layers.batch_norm,\n",
    "        normalizer_params={'is_training': is_training,\n",
    "                           'updates_collections': None}):\n",
    "    \n",
    "        top = data\n",
    "        top = layers.conv2d(top, 32, [3, 3], stride=2, normalizer_fn=None)\n",
    "        top = layers.conv2d(top, 32, [3, 3], stride=1)\n",
    "        top = layers.conv2d(top, 64, [3, 3], stride=2)\n",
    "        top = layers.conv2d(top, 64, [3, 3], stride=1)\n",
    "        top = layers.conv2d(top, 128, [3, 3], stride=2)\n",
    "        top = layers.conv2d(top, 128, [3, 3], stride=1)\n",
    "        top = layers.conv2d(top, 256, [3, 3], stride=2)\n",
    "        top = layers.conv2d(top, 256, [3, 3], stride=1)\n",
    "        top = layers.conv2d(top, 512, [3, 3], stride=2)\n",
    "        top = layers.conv2d(top, 512, [3, 3], stride=1)\n",
    "        top = layers.flatten(top)\n",
    "        \n",
    "        data_logit = util.linear(top, 1)\n",
    "        \n",
    "        return data_logit\n",
    "    \n",
    "\n",
    "train_dir = 'celeba_hresgan_logs'\n",
    "shutil.rmtree(train_dir, ignore_errors=True)\n",
    "batch_size = 8\n",
    "image_size = 256\n",
    "noise_dim = 100\n",
    "with tf.Graph().as_default():\n",
    "    dataset = celeba_dataset.Dataset('../CelebA', image_size)\n",
    "    iterator = dataset.make_iterator(batch_size=batch_size)\n",
    "    next_elem = iterator.get_next()\n",
    "    \n",
    "    real_data = next_elem.pop(celeba_dataset.IMAGE_KEY)\n",
    "    real_data.set_shape((batch_size, image_size, image_size, 3))\n",
    "    noise = tf.random_normal(shape=(batch_size, noise_dim))\n",
    "    \n",
    "    # Build model\n",
    "    with tf.variable_scope('generator') as gen_scope:\n",
    "        gen_data = make_generator(noise, is_training=True)\n",
    "    \n",
    "    with tf.variable_scope('discriminator') as dis_scope:\n",
    "        dis_gen_logit = make_discriminator(\n",
    "            gen_data, is_training=True)\n",
    "        \n",
    "    with tf.variable_scope(dis_scope, reuse=True):\n",
    "        dis_real_logit = make_discriminator(\n",
    "            real_data, is_training=True)\n",
    "    \n",
    "    with tf.variable_scope(gen_scope, reuse=True):\n",
    "        test_noise = tf.placeholder(dtype=tf.float32, shape=(None, noise_dim))\n",
    "        test_data = make_generator(test_noise, is_training=False)\n",
    "    \n",
    "    # Add test variables to collections\n",
    "    tf.add_to_collection('celeba/noise', test_noise)\n",
    "    tf.add_to_collection('celeba/data', test_data)\n",
    "\n",
    "    # Loss\n",
    "    gen_loss = -tf.losses.sigmoid_cross_entropy(\n",
    "        tf.zeros_like(dis_gen_logit), dis_gen_logit)\n",
    "    \n",
    "    dis_loss = (\n",
    "        tf.losses.sigmoid_cross_entropy(\n",
    "            tf.zeros_like(dis_gen_logit), dis_gen_logit) +\n",
    "        tf.losses.sigmoid_cross_entropy(\n",
    "            tf.ones_like(dis_real_logit), dis_real_logit)\n",
    "    )\n",
    "    \n",
    "    # Train ops\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    \n",
    "    gen_vars = util.get_trainable_variables_in_scope(gen_scope)\n",
    "    dis_vars = util.get_trainable_variables_in_scope(dis_scope)\n",
    "    \n",
    "    optim = tf.train.AdamOptimizer(1.0e-5, beta1=0.5, beta2=0.99)\n",
    "    gen_train_op = optim.minimize(\n",
    "        loss=gen_loss, var_list=gen_vars, global_step=global_step)\n",
    "    \n",
    "    optim = tf.train.AdamOptimizer(1.0e-5, beta1=0.5, beta2=0.99)\n",
    "    dis_train_op = optim.minimize(\n",
    "        loss=dis_loss, var_list=dis_vars, global_step=global_step)\n",
    "        \n",
    "    tf.summary.scalar('gen_loss', gen_loss)\n",
    "    tf.summary.scalar('dis_loss', dis_loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer(), feed_dict=dataset.feed_dict)\n",
    "        sess.run(iterator.initializer, feed_dict=dataset.feed_dict)\n",
    "        \n",
    "        summary_writer = tf.summary.FileWriter(\n",
    "            logdir=train_dir, graph=sess.graph)\n",
    "        \n",
    "        with util.TensorflowQueues(sess):\n",
    "            step = 0\n",
    "            gen_step = 0\n",
    "            dis_step = 0\n",
    "            dis_loss_value = 0.0\n",
    "            gen_loss_value = 0.0\n",
    "            while step < 1000000:\n",
    "                if gen_loss_value < -np.log(2.0):\n",
    "                    _, dis_loss_value = sess.run([dis_train_op, dis_loss])\n",
    "                    dis_step += 1\n",
    "                if dis_loss_value < 2.0 * np.log(2.0):\n",
    "                    _, gen_loss_value = sess.run([gen_train_op, gen_loss])\n",
    "                    gen_step += 1\n",
    "            \n",
    "                step = sess.run(global_step)\n",
    "                if step % 100 == 0:\n",
    "                    print ('Global step {}: gen_loss = {}, dis_loss = {}, '\n",
    "                           'gen_step = {}, dis_step = {}').format(\n",
    "                        step, gen_loss_value, dis_loss_value, gen_step, dis_step)\n",
    "                    \n",
    "                    print 'Writing summaries'\n",
    "                    summary_proto = sess.run(summary_op)\n",
    "                    summary_writer.add_summary(summary_proto, global_step=step)\n",
    "                if step % 1000 == 0:\n",
    "                    print 'Writing checkpoint'\n",
    "                    saver.save(sess, os.path.join(train_dir, 'model'), global_step=step)\n",
    "                    \n",
    "                    feed_dict = {\n",
    "                        test_noise: np.random.normal(size=(5, noise_dim))\n",
    "                    }\n",
    "                    test_data_value = sess.run(test_data, feed_dict=feed_dict)\n",
    "                    util.plot_rgb_images(test_data_value)\n",
    "                    \n",
    "                    print '='*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
